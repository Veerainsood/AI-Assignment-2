<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gym_vrp.envs.tsp API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gym_vrp.envs.tsp</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Tuple, Union

import numpy as np
from gym import Env
from gym.wrappers.monitoring.video_recorder import VideoRecorder

from ..graph.vrp_network import VRPNetwork
from .common import ObsType


class TSPEnv(Env):
    &#34;&#34;&#34;
    TSPEnv implements the Traveling Salesmen Problem
    a special variant of the vehicle routing problem.

    State: Shape (batch_size, num_nodes, 4) The third
        dimension is structured as follows:
        [x_coord, y_coord, is_depot, visitable]

    Actions: Depends on the number of nodes in every graph.
        Should contain the node numbers to visit next for
        each graph. Shape (batch_size, 1)
    &#34;&#34;&#34;

    metadata = {&#34;render.modes&#34;: [&#34;human&#34;, &#34;rgb_array&#34;]}

    def __init__(
        self,
        num_nodes: int = 20,
        batch_size: int = 128,
        num_draw: int = 6,
        seed: int = 69,
    ):
        &#34;&#34;&#34;
        Args:
            num_nodes (int, optional): Number of nodes in each generated graph. Defaults to 32.
            batch_size (int, optional): Number of graphs to generate. Defaults to 128.
            num_draw (int, optional): When calling the render num_draw graphs will be rendered. 
                Defaults to 6.
            seed (int, optional): Seed of the environment. Defaults to 69.
            video_save_path (str, optional): When set a video of the interactions with the 
                environment is saved at the set location. Defaults to None.
        &#34;&#34;&#34;
        assert (
            num_draw &lt;= batch_size
        ), &#34;Num_draw needs to be equal or lower than the number of generated graphs.&#34;

        np.random.seed(seed)

        self.step_count = 0
        self.num_nodes = num_nodes
        self.batch_size = batch_size

        # init video recorder
        self.draw_idxs = np.random.choice(batch_size, num_draw, replace=False)
        self.video_save_path = None

        self.generate_graphs()

    def step(self, actions: np.ndarray) -&gt; Tuple[ObsType, float, bool, dict]:
        &#34;&#34;&#34;
        Run the environment one timestep. It&#39;s the users responsiblity to
        call reset() when the end of the episode has been reached. Accepts
        an actions and return a tuple of (observation, reward, done, info)

        Args:
            actions (nd.ndarray): Which node to visit for each graph.
                Shape of actions is (batch_size, 1).

        Returns:
            Tuple[ObsType, float, bool, dict]: Tuple of the observation,
                reward, done and info. The observation is within
                self.observation_space. The reward is for the previous action.
                If done equals True then the episode is over. Stepping through
                environment while done returns undefined results. Info contains
                may contain additions info in terms of metrics, state variables
                and such.
        &#34;&#34;&#34;
        assert (
            actions.shape[0] == self.batch_size
        ), &#34;Number of actions need to equal the number of generated graphs.&#34;

        self.step_count += 1

        # visit each next node
        self.visited[np.arange(len(actions)), actions.T] = 1
        traversed_edges = np.hstack([self.current_location, actions]).astype(int)
        self.sampler.visit_edges(traversed_edges)

        self.current_location = np.array(actions)

        if self.video_save_path is not None:
            self.vid.capture_frame()

        done = self.is_done()
        return (
            self.get_state(),
            -self.sampler.get_distances(traversed_edges),
            done,
            None,
        )

    def is_done(self):
        return np.all(self.visited == 1)

    def get_state(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Getter for the current environment state

        Returns:
            np.ndarray: Shape (num_graph, num_nodes, 4)
            where the third dimension consists of the
            x, y coordinates, if the node is a depot,
            and if it has been visted yet.
        &#34;&#34;&#34;

        # generate state (depots not yet set)
        state = np.dstack(
            [
                self.sampler.get_graph_positions(),
                np.zeros((self.batch_size, self.num_nodes)),
                self.generate_mask(),
            ]
        )

        # set depots in state to 1
        state[np.arange(len(state)), self.depots.T, 2] = 1

        return state

    def generate_mask(self):
        &#34;&#34;&#34;
        Generates a mask of where the nodes marked as 1 cannot 
        be visited in the next step according to the env dynamic.

        Returns:
            np.ndarray: Returns mask for each (un)visitable node 
                in each graph. Shape (batch_size, num_nodes)
        &#34;&#34;&#34;
        # disallow staying on a depot
        depot_graphs_idxs = np.where(self.current_location == self.depots)[0]
        self.visited[depot_graphs_idxs, self.depots[depot_graphs_idxs].squeeze()] = 1

        # allow staying on a depot if the graph is solved.
        done_graphs = np.where(np.all(self.visited, axis=1) == True)[0]
        self.visited[done_graphs, self.depots[done_graphs].squeeze()] = 0

        return self.visited

    def reset(self) -&gt; Union[ObsType, Tuple[ObsType, dict]]:
        &#34;&#34;&#34;
        Resets the environment. 

        Returns:
            Union[ObsType, Tuple[ObsType, dict]]: State of the environment.
        &#34;&#34;&#34;

        self.step_count = 0
        self.generate_graphs()
        return self.get_state()

    def generate_graphs(self):
        &#34;&#34;&#34;
        Generates a VRPNetwork of batch_size graphs with num_nodes
        each. Resets the visited nodes to 0.
        &#34;&#34;&#34;
        self.visited = np.zeros(shape=(self.batch_size, self.num_nodes))
        self.sampler = VRPNetwork(
            num_graphs=self.batch_size, num_nodes=self.num_nodes, num_depots=1,
        )

        # set current location to the depots
        self.depots = self.sampler.get_depots()
        self.current_location = self.depots

    def render(self, mode: str = &#34;human&#34;):
        &#34;&#34;&#34;
        Visualize one step in the env. Since its batched 
        this methods renders n random graphs from the batch.
        &#34;&#34;&#34;
        return self.sampler.draw(self.draw_idxs)

    def enable_video_capturing(self, video_save_path: str):
        self.video_save_path = video_save_path
        if self.video_save_path is not None:
            self.vid = VideoRecorder(self, self.video_save_path)
            self.vid.frames_per_sec = 1</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gym_vrp.envs.tsp.TSPEnv"><code class="flex name class">
<span>class <span class="ident">TSPEnv</span></span>
<span>(</span><span>num_nodes: int = 20, batch_size: int = 128, num_draw: int = 6, seed: int = 69)</span>
</code></dt>
<dd>
<div class="desc"><p>TSPEnv implements the Traveling Salesmen Problem
a special variant of the vehicle routing problem.</p>
<p>State: Shape (batch_size, num_nodes, 4) The third
dimension is structured as follows:
[x_coord, y_coord, is_depot, visitable]</p>
<p>Actions: Depends on the number of nodes in every graph.
Should contain the node numbers to visit next for
each graph. Shape (batch_size, 1)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_nodes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of nodes in each generated graph. Defaults to 32.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of graphs to generate. Defaults to 128.</dd>
<dt><strong><code>num_draw</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>When calling the render num_draw graphs will be rendered.
Defaults to 6.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Seed of the environment. Defaults to 69.</dd>
<dt><strong><code>video_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>When set a video of the interactions with the
environment is saved at the set location. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TSPEnv(Env):
    &#34;&#34;&#34;
    TSPEnv implements the Traveling Salesmen Problem
    a special variant of the vehicle routing problem.

    State: Shape (batch_size, num_nodes, 4) The third
        dimension is structured as follows:
        [x_coord, y_coord, is_depot, visitable]

    Actions: Depends on the number of nodes in every graph.
        Should contain the node numbers to visit next for
        each graph. Shape (batch_size, 1)
    &#34;&#34;&#34;

    metadata = {&#34;render.modes&#34;: [&#34;human&#34;, &#34;rgb_array&#34;]}

    def __init__(
        self,
        num_nodes: int = 20,
        batch_size: int = 128,
        num_draw: int = 6,
        seed: int = 69,
    ):
        &#34;&#34;&#34;
        Args:
            num_nodes (int, optional): Number of nodes in each generated graph. Defaults to 32.
            batch_size (int, optional): Number of graphs to generate. Defaults to 128.
            num_draw (int, optional): When calling the render num_draw graphs will be rendered. 
                Defaults to 6.
            seed (int, optional): Seed of the environment. Defaults to 69.
            video_save_path (str, optional): When set a video of the interactions with the 
                environment is saved at the set location. Defaults to None.
        &#34;&#34;&#34;
        assert (
            num_draw &lt;= batch_size
        ), &#34;Num_draw needs to be equal or lower than the number of generated graphs.&#34;

        np.random.seed(seed)

        self.step_count = 0
        self.num_nodes = num_nodes
        self.batch_size = batch_size

        # init video recorder
        self.draw_idxs = np.random.choice(batch_size, num_draw, replace=False)
        self.video_save_path = None

        self.generate_graphs()

    def step(self, actions: np.ndarray) -&gt; Tuple[ObsType, float, bool, dict]:
        &#34;&#34;&#34;
        Run the environment one timestep. It&#39;s the users responsiblity to
        call reset() when the end of the episode has been reached. Accepts
        an actions and return a tuple of (observation, reward, done, info)

        Args:
            actions (nd.ndarray): Which node to visit for each graph.
                Shape of actions is (batch_size, 1).

        Returns:
            Tuple[ObsType, float, bool, dict]: Tuple of the observation,
                reward, done and info. The observation is within
                self.observation_space. The reward is for the previous action.
                If done equals True then the episode is over. Stepping through
                environment while done returns undefined results. Info contains
                may contain additions info in terms of metrics, state variables
                and such.
        &#34;&#34;&#34;
        assert (
            actions.shape[0] == self.batch_size
        ), &#34;Number of actions need to equal the number of generated graphs.&#34;

        self.step_count += 1

        # visit each next node
        self.visited[np.arange(len(actions)), actions.T] = 1
        traversed_edges = np.hstack([self.current_location, actions]).astype(int)
        self.sampler.visit_edges(traversed_edges)

        self.current_location = np.array(actions)

        if self.video_save_path is not None:
            self.vid.capture_frame()

        done = self.is_done()
        return (
            self.get_state(),
            -self.sampler.get_distances(traversed_edges),
            done,
            None,
        )

    def is_done(self):
        return np.all(self.visited == 1)

    def get_state(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Getter for the current environment state

        Returns:
            np.ndarray: Shape (num_graph, num_nodes, 4)
            where the third dimension consists of the
            x, y coordinates, if the node is a depot,
            and if it has been visted yet.
        &#34;&#34;&#34;

        # generate state (depots not yet set)
        state = np.dstack(
            [
                self.sampler.get_graph_positions(),
                np.zeros((self.batch_size, self.num_nodes)),
                self.generate_mask(),
            ]
        )

        # set depots in state to 1
        state[np.arange(len(state)), self.depots.T, 2] = 1

        return state

    def generate_mask(self):
        &#34;&#34;&#34;
        Generates a mask of where the nodes marked as 1 cannot 
        be visited in the next step according to the env dynamic.

        Returns:
            np.ndarray: Returns mask for each (un)visitable node 
                in each graph. Shape (batch_size, num_nodes)
        &#34;&#34;&#34;
        # disallow staying on a depot
        depot_graphs_idxs = np.where(self.current_location == self.depots)[0]
        self.visited[depot_graphs_idxs, self.depots[depot_graphs_idxs].squeeze()] = 1

        # allow staying on a depot if the graph is solved.
        done_graphs = np.where(np.all(self.visited, axis=1) == True)[0]
        self.visited[done_graphs, self.depots[done_graphs].squeeze()] = 0

        return self.visited

    def reset(self) -&gt; Union[ObsType, Tuple[ObsType, dict]]:
        &#34;&#34;&#34;
        Resets the environment. 

        Returns:
            Union[ObsType, Tuple[ObsType, dict]]: State of the environment.
        &#34;&#34;&#34;

        self.step_count = 0
        self.generate_graphs()
        return self.get_state()

    def generate_graphs(self):
        &#34;&#34;&#34;
        Generates a VRPNetwork of batch_size graphs with num_nodes
        each. Resets the visited nodes to 0.
        &#34;&#34;&#34;
        self.visited = np.zeros(shape=(self.batch_size, self.num_nodes))
        self.sampler = VRPNetwork(
            num_graphs=self.batch_size, num_nodes=self.num_nodes, num_depots=1,
        )

        # set current location to the depots
        self.depots = self.sampler.get_depots()
        self.current_location = self.depots

    def render(self, mode: str = &#34;human&#34;):
        &#34;&#34;&#34;
        Visualize one step in the env. Since its batched 
        this methods renders n random graphs from the batch.
        &#34;&#34;&#34;
        return self.sampler.draw(self.draw_idxs)

    def enable_video_capturing(self, video_save_path: str):
        self.video_save_path = video_save_path
        if self.video_save_path is not None:
            self.vid = VideoRecorder(self, self.video_save_path)
            self.vid.frames_per_sec = 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Env</li>
<li>typing.Generic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="gym_vrp.envs.irp.IRPEnv" href="irp.html#gym_vrp.envs.irp.IRPEnv">IRPEnv</a></li>
<li><a title="gym_vrp.envs.vrp.VRPEnv" href="vrp.html#gym_vrp.envs.vrp.VRPEnv">VRPEnv</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gym_vrp.envs.tsp.TSPEnv.action_space"><code class="name">var <span class="ident">action_space</span> : spaces.Space[ActType]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.metadata"><code class="name">var <span class="ident">metadata</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.observation_space"><code class="name">var <span class="ident">observation_space</span> : spaces.Space[ObsType]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gym_vrp.envs.tsp.TSPEnv.enable_video_capturing"><code class="name flex">
<span>def <span class="ident">enable_video_capturing</span></span>(<span>self, video_save_path: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enable_video_capturing(self, video_save_path: str):
    self.video_save_path = video_save_path
    if self.video_save_path is not None:
        self.vid = VideoRecorder(self, self.video_save_path)
        self.vid.frames_per_sec = 1</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.generate_graphs"><code class="name flex">
<span>def <span class="ident">generate_graphs</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a VRPNetwork of batch_size graphs with num_nodes
each. Resets the visited nodes to 0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_graphs(self):
    &#34;&#34;&#34;
    Generates a VRPNetwork of batch_size graphs with num_nodes
    each. Resets the visited nodes to 0.
    &#34;&#34;&#34;
    self.visited = np.zeros(shape=(self.batch_size, self.num_nodes))
    self.sampler = VRPNetwork(
        num_graphs=self.batch_size, num_nodes=self.num_nodes, num_depots=1,
    )

    # set current location to the depots
    self.depots = self.sampler.get_depots()
    self.current_location = self.depots</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.generate_mask"><code class="name flex">
<span>def <span class="ident">generate_mask</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a mask of where the nodes marked as 1 cannot
be visited in the next step according to the env dynamic.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Returns mask for each (un)visitable node
in each graph. Shape (batch_size, num_nodes)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_mask(self):
    &#34;&#34;&#34;
    Generates a mask of where the nodes marked as 1 cannot 
    be visited in the next step according to the env dynamic.

    Returns:
        np.ndarray: Returns mask for each (un)visitable node 
            in each graph. Shape (batch_size, num_nodes)
    &#34;&#34;&#34;
    # disallow staying on a depot
    depot_graphs_idxs = np.where(self.current_location == self.depots)[0]
    self.visited[depot_graphs_idxs, self.depots[depot_graphs_idxs].squeeze()] = 1

    # allow staying on a depot if the graph is solved.
    done_graphs = np.where(np.all(self.visited, axis=1) == True)[0]
    self.visited[done_graphs, self.depots[done_graphs].squeeze()] = 0

    return self.visited</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.get_state"><code class="name flex">
<span>def <span class="ident">get_state</span></span>(<span>self) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Getter for the current environment state</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Shape (num_graph, num_nodes, 4)</dd>
</dl>
<p>where the third dimension consists of the
x, y coordinates, if the node is a depot,
and if it has been visted yet.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_state(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Getter for the current environment state

    Returns:
        np.ndarray: Shape (num_graph, num_nodes, 4)
        where the third dimension consists of the
        x, y coordinates, if the node is a depot,
        and if it has been visted yet.
    &#34;&#34;&#34;

    # generate state (depots not yet set)
    state = np.dstack(
        [
            self.sampler.get_graph_positions(),
            np.zeros((self.batch_size, self.num_nodes)),
            self.generate_mask(),
        ]
    )

    # set depots in state to 1
    state[np.arange(len(state)), self.depots.T, 2] = 1

    return state</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.is_done"><code class="name flex">
<span>def <span class="ident">is_done</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_done(self):
    return np.all(self.visited == 1)</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self, mode: str = 'human')</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize one step in the env. Since its batched
this methods renders n random graphs from the batch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render(self, mode: str = &#34;human&#34;):
    &#34;&#34;&#34;
    Visualize one step in the env. Since its batched 
    this methods renders n random graphs from the batch.
    &#34;&#34;&#34;
    return self.sampler.draw(self.draw_idxs)</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) ‑> Union[~ObsType, Tuple[~ObsType, dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment. </p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[ObsType, Tuple[ObsType, dict]]</code></dt>
<dd>State of the environment.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; Union[ObsType, Tuple[ObsType, dict]]:
    &#34;&#34;&#34;
    Resets the environment. 

    Returns:
        Union[ObsType, Tuple[ObsType, dict]]: State of the environment.
    &#34;&#34;&#34;

    self.step_count = 0
    self.generate_graphs()
    return self.get_state()</code></pre>
</details>
</dd>
<dt id="gym_vrp.envs.tsp.TSPEnv.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, actions: numpy.ndarray) ‑> Tuple[~ObsType, float, bool, dict]</span>
</code></dt>
<dd>
<div class="desc"><p>Run the environment one timestep. It's the users responsiblity to
call reset() when the end of the episode has been reached. Accepts
an actions and return a tuple of (observation, reward, done, info)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>actions</code></strong> :&ensp;<code>nd.ndarray</code></dt>
<dd>Which node to visit for each graph.
Shape of actions is (batch_size, 1).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[ObsType, float, bool, dict]</code></dt>
<dd>Tuple of the observation,
reward, done and info. The observation is within
self.observation_space. The reward is for the previous action.
If done equals True then the episode is over. Stepping through
environment while done returns undefined results. Info contains
may contain additions info in terms of metrics, state variables
and such.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, actions: np.ndarray) -&gt; Tuple[ObsType, float, bool, dict]:
    &#34;&#34;&#34;
    Run the environment one timestep. It&#39;s the users responsiblity to
    call reset() when the end of the episode has been reached. Accepts
    an actions and return a tuple of (observation, reward, done, info)

    Args:
        actions (nd.ndarray): Which node to visit for each graph.
            Shape of actions is (batch_size, 1).

    Returns:
        Tuple[ObsType, float, bool, dict]: Tuple of the observation,
            reward, done and info. The observation is within
            self.observation_space. The reward is for the previous action.
            If done equals True then the episode is over. Stepping through
            environment while done returns undefined results. Info contains
            may contain additions info in terms of metrics, state variables
            and such.
    &#34;&#34;&#34;
    assert (
        actions.shape[0] == self.batch_size
    ), &#34;Number of actions need to equal the number of generated graphs.&#34;

    self.step_count += 1

    # visit each next node
    self.visited[np.arange(len(actions)), actions.T] = 1
    traversed_edges = np.hstack([self.current_location, actions]).astype(int)
    self.sampler.visit_edges(traversed_edges)

    self.current_location = np.array(actions)

    if self.video_save_path is not None:
        self.vid.capture_frame()

    done = self.is_done()
    return (
        self.get_state(),
        -self.sampler.get_distances(traversed_edges),
        done,
        None,
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gym_vrp.envs" href="index.html">gym_vrp.envs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gym_vrp.envs.tsp.TSPEnv" href="#gym_vrp.envs.tsp.TSPEnv">TSPEnv</a></code></h4>
<ul class="">
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.action_space" href="#gym_vrp.envs.tsp.TSPEnv.action_space">action_space</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.enable_video_capturing" href="#gym_vrp.envs.tsp.TSPEnv.enable_video_capturing">enable_video_capturing</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.generate_graphs" href="#gym_vrp.envs.tsp.TSPEnv.generate_graphs">generate_graphs</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.generate_mask" href="#gym_vrp.envs.tsp.TSPEnv.generate_mask">generate_mask</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.get_state" href="#gym_vrp.envs.tsp.TSPEnv.get_state">get_state</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.is_done" href="#gym_vrp.envs.tsp.TSPEnv.is_done">is_done</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.metadata" href="#gym_vrp.envs.tsp.TSPEnv.metadata">metadata</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.observation_space" href="#gym_vrp.envs.tsp.TSPEnv.observation_space">observation_space</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.render" href="#gym_vrp.envs.tsp.TSPEnv.render">render</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.reset" href="#gym_vrp.envs.tsp.TSPEnv.reset">reset</a></code></li>
<li><code><a title="gym_vrp.envs.tsp.TSPEnv.step" href="#gym_vrp.envs.tsp.TSPEnv.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>